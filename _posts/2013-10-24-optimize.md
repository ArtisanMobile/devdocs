---
layout: docs
title: "Optimize"
author: "Artisan"
category: user-guide
description: "Getting started with the Artisan MEM platform for developers."
---
#Artisan Optimize

##Getting Started
In order to get the most out of Optimize, start by deciding on your most important key performance indicators, or KPI, such as sign up rate or conversion rate or time in app. Once you've identified the KPI that matters most, you can focus your testing effort on that particular area. An A/B test that aims to improve the sign up rate would focus on testing different layouts of the sign in screen; to improve conversion rate would experiment with different call to actions on the check out screen; to increase time in app you might modify the user's navigation flow. To learn more about A/B testing please see our A/B Testing Best Practices section.

Artisan Optimize is the first solution that enables real-time A/B testing of existing native mobile applications without writing code or re-submitting to app stores. Business professionals such as marketers, designers and product managers can now test and optimize designs in a matter of minutes versus weeks with a traditional coding approach. Just like websites, making small changes in mobile app experiences can have big results.

##What is A/B Testing?

A/B testing, also known as split testing, is a method of conversion rate optimization. It involves creating a set of different variations that are randomly presented to the customer base at the same time. The goal is to identify which particular designs and changes correlate to an increase in conversions. With A/B testing, you can send out experiments to your user base to find measurable results on which designs resonate with your customers.

##Creating your First Experiment

Starting in App Map, select the screen that you would like to experiment on. 

Click on **Create a New Experiment**, this will take you to the design canvas. 

From the variations tab you can easily create new variations and design screen designs. For more information please search key words Design or Variations on our GetSatisfaction forum by clicking here. 

Select **Variation** on the left sidebar and name it. You will now be able to build the first variation for the A/B experiment on the selected page. Select different UI elements as noted in the left column. Each element should automatically be highlighted in your app on the device or simulator. Make an edit to one of the properties noted in the right column for the selected UI element. Note how the edited property is immediately reflected in your app on the device and in the center screen area. Modify the screen to build a sample UI alternative that you wish to experiment as part of an A/B experiment. All elements displayed on the screen can be edited. 

For additional variations to be included in the A/B experiment, click the **+** button on the left sidebar. An additional variation should emerge at the top of the left sidebar. In selecting this additional variation, you should note that it immediately takes effect in the app on the device. After you have designed your screens press Ready to Test in the upper right hand corner. 

Now from the Configure page, you set the parameters of your experiment such as duration of the experiment. 

To set your target, navigate through your app to click the end target that you wish to use as the end of your funnel. This target can be anywhere on your app, even on screens that are not being A/B experimented. Notice how each time a target is clicked in the app on the device, this is detected automatically and displayed in the Set Target dialog box. 

Enter the number of users to be included in the test pool (maximum 10,000) and the distribution of the variations across the test pool - more information is available by searching for Distribution in the Customer Community. 

When completed click save then at the top right side click Start. Your Experiment is now started. You can refer to the Insights tab to see status of the experiment. Allow sufficient time for data to be populated.

##Understanding Your Experiment Results

At Artisan, we use confidence levels to explain the results of A/B Tests. Of course we show you the various conversion rates, number of views and users, but the main mechanism used at Artisan to provide deep insight and validity to the test results is what is known as confidence levels.

A confidence level signifies the amount of certainty that the conversion rate of a variation within an experiment will beat the conversion rate of the control when published to the total population of app users. 

The industry standard is to achieve a 95% confidence level. This means that there is a 95% chance that the conversion rate of the variation will be higher than that of the control when published. Artisan signals that 95% confidence has been achieved by adding a yellow/green medal to the variation on the experiment screen. For the exact confidence level, hover over the yellow/green medal or download the detailed statistics for the experiment. 

A yellow medal indicates that the particular variation has a 95% confidence level that its conversion rate is better than the control. A green medal indicates that the particular variation has a 95% confidence level that its conversion rate is better than all other variation. 

What this means is that we are 95% sure that a given variation performing better than another is not some fluke, and that the results are reproducible (i.e when published, these variations will behave the same in the real world). 

For more detailed information on confidence levels, refer to [https://getsatisfaction.com/artisan/topics/how_does_confidence_level_relate_to_a_b_testing](https://getsatisfaction.com/artisan/topics/how_does_confidence_level_relate_to_a_b_testing).





